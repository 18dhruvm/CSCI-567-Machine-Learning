{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "K means with K++ and without PCA"
      ],
      "metadata": {
        "id": "LC755igP9sYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3FyoOkB9mFq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/final-df/final_df.csv')\n",
        "\n",
        "features = data.drop(columns=[\"id\", \"score\"])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "k = 12\n",
        "\n",
        "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(features_standardized)\n",
        "\n",
        "min_score = 0.5\n",
        "step_size = 0.5\n",
        "score_mapping = {i: min_score + i * step_size for i in range(k)}\n",
        "data['predicted_score'] = data['cluster'].map(score_mapping)\n",
        "\n",
        "actual_scores = data['score']\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(actual_scores, data['predicted_score']))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K means with K++ and PCA"
      ],
      "metadata": {
        "id": "P3ibNvef936J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/final-df/final_df.csv')\n",
        "\n",
        "\n",
        "features = data.drop(columns=[\"id\", \"score\"])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "features_pca = pca.fit_transform(features_standardized)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "k = 12\n",
        "\n",
        "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(features_pca)\n",
        "\n",
        "min_score = 0.5\n",
        "step_size = 0.5\n",
        "score_mapping = {i: min_score + i * step_size for i in range(k)}\n",
        "data['predicted_score'] = data['cluster'].map(score_mapping)\n",
        "\n",
        "actual_scores = data['score']\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(actual_scores, data['predicted_score']))\n",
        "print(f\"RMSE: {rmse}\")"
      ],
      "metadata": {
        "id": "AyXyzw8s92TH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}