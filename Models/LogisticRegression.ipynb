{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QuuIFZJUr1QC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final1=pd.read_csv('/content/drive/Shareddrives/CSCI567/CSCI567/writing_quality/final_df.csv')\n",
        "merged_data= pd.read_csv('/content/drive/Shareddrives/CSCI567/CSCI567/writing_quality/train_logs.csv')\n",
        "\n",
        "numerical_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
        "df_numerical_agg = merged_data.groupby('id')[numerical_cols].mean().reset_index()\n",
        "\n",
        "# Handling categorical columns by getting the count of unique values\n",
        "categorical_cols = ['activity', 'down_event', 'up_event', 'text_change']\n",
        "df_categorical_agg = merged_data.groupby('id')[categorical_cols].agg(lambda x: x.value_counts().max()).reset_index()\n",
        "\n",
        "# Merging the aggregated numerical and categorical DataFrames\n",
        "df_final = pd.merge(df_numerical_agg, df_categorical_agg, on='id', how=\"outer\")\n",
        "\n",
        "# Merging the aggregated DataFrame with the scores DataFrame to get the labels\n",
        "df_final = pd.merge(df_final, df_final1, on='id', how=\"outer\")\n",
        "\n",
        "df_final.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "c6KEw4N8r3HY",
        "outputId": "07364482-9870-42bc-8ef8-c5dd4df28bce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id      down_time        up_time  action_time  cursor_position  \\\n",
              "0  001519c8  848180.771998  848297.018772   116.246774       711.163473   \n",
              "1  0022f953  518855.347596  518967.568867   112.221271       776.205786   \n",
              "2  0042269b  828491.775145  828593.612911   101.837766       731.611702   \n",
              "3  0059420b  785483.026350  785604.874679   121.848329       542.537275   \n",
              "4  0075873a  713354.197550  713478.141446   123.943896       600.050968   \n",
              "\n",
              "   word_count  activity  down_event  up_event  text_change  ...  \\\n",
              "0  128.116152      2010        1619      1619         1940  ...   \n",
              "1  182.714751      1938        1490      1490         1698  ...   \n",
              "2  194.772727      3515        2904      2899         3257  ...   \n",
              "3  103.618895      1304        1038      1038         1146  ...   \n",
              "4  125.082971      1942        1541      1541         1964  ...   \n",
              "\n",
              "   Total_Pause_Time_within_word  Mean_Pause_Time_within_word  \\\n",
              "0                        685007                  1918.787115   \n",
              "1                       1087710                  2781.867008   \n",
              "2                        843679                  1528.403986   \n",
              "3                        745392                  3067.456790   \n",
              "4                       1039641                  3208.768519   \n",
              "\n",
              "   SD_Pause_Time_within_word  Total_Pause_Time_Before_Sentences  \\\n",
              "0                7273.768138                               3643   \n",
              "1               16954.357992                              69412   \n",
              "2                9793.226830                              87089   \n",
              "3               13400.325753                            1466851   \n",
              "4               11181.242502                             142133   \n",
              "\n",
              "   Mean_Pause_Time_Before_Sentences  SD_Pause_Time_Before_Sentences  \\\n",
              "0                        173.476190                      416.665168   \n",
              "1                       4627.466667                     8278.455203   \n",
              "2                       4147.095238                    15826.690943   \n",
              "3                     112834.692308                   365509.369147   \n",
              "4                       6179.695652                    13762.720184   \n",
              "\n",
              "   Total_Pause_Time_in_sentences  Mean_Pause_Time_in_sentences  \\\n",
              "0                        1181819                  56277.095238   \n",
              "1                         581673                  38778.200000   \n",
              "2                        1259632                  59982.476190   \n",
              "3                        1032626                  79432.769231   \n",
              "4                        1073221                  46661.782609   \n",
              "\n",
              "   SD_Pause_Time_in_sentences  score  \n",
              "0                43455.913672    3.5  \n",
              "1                32758.883810    3.5  \n",
              "2                58697.726407    6.0  \n",
              "3                48355.056728    2.0  \n",
              "4                73950.739406    4.0  \n",
              "\n",
              "[5 rows x 60 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e8fb943-9174-468f-955f-0efcfe0c5cb7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>down_time</th>\n",
              "      <th>up_time</th>\n",
              "      <th>action_time</th>\n",
              "      <th>cursor_position</th>\n",
              "      <th>word_count</th>\n",
              "      <th>activity</th>\n",
              "      <th>down_event</th>\n",
              "      <th>up_event</th>\n",
              "      <th>text_change</th>\n",
              "      <th>...</th>\n",
              "      <th>Total_Pause_Time_within_word</th>\n",
              "      <th>Mean_Pause_Time_within_word</th>\n",
              "      <th>SD_Pause_Time_within_word</th>\n",
              "      <th>Total_Pause_Time_Before_Sentences</th>\n",
              "      <th>Mean_Pause_Time_Before_Sentences</th>\n",
              "      <th>SD_Pause_Time_Before_Sentences</th>\n",
              "      <th>Total_Pause_Time_in_sentences</th>\n",
              "      <th>Mean_Pause_Time_in_sentences</th>\n",
              "      <th>SD_Pause_Time_in_sentences</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001519c8</td>\n",
              "      <td>848180.771998</td>\n",
              "      <td>848297.018772</td>\n",
              "      <td>116.246774</td>\n",
              "      <td>711.163473</td>\n",
              "      <td>128.116152</td>\n",
              "      <td>2010</td>\n",
              "      <td>1619</td>\n",
              "      <td>1619</td>\n",
              "      <td>1940</td>\n",
              "      <td>...</td>\n",
              "      <td>685007</td>\n",
              "      <td>1918.787115</td>\n",
              "      <td>7273.768138</td>\n",
              "      <td>3643</td>\n",
              "      <td>173.476190</td>\n",
              "      <td>416.665168</td>\n",
              "      <td>1181819</td>\n",
              "      <td>56277.095238</td>\n",
              "      <td>43455.913672</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022f953</td>\n",
              "      <td>518855.347596</td>\n",
              "      <td>518967.568867</td>\n",
              "      <td>112.221271</td>\n",
              "      <td>776.205786</td>\n",
              "      <td>182.714751</td>\n",
              "      <td>1938</td>\n",
              "      <td>1490</td>\n",
              "      <td>1490</td>\n",
              "      <td>1698</td>\n",
              "      <td>...</td>\n",
              "      <td>1087710</td>\n",
              "      <td>2781.867008</td>\n",
              "      <td>16954.357992</td>\n",
              "      <td>69412</td>\n",
              "      <td>4627.466667</td>\n",
              "      <td>8278.455203</td>\n",
              "      <td>581673</td>\n",
              "      <td>38778.200000</td>\n",
              "      <td>32758.883810</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0042269b</td>\n",
              "      <td>828491.775145</td>\n",
              "      <td>828593.612911</td>\n",
              "      <td>101.837766</td>\n",
              "      <td>731.611702</td>\n",
              "      <td>194.772727</td>\n",
              "      <td>3515</td>\n",
              "      <td>2904</td>\n",
              "      <td>2899</td>\n",
              "      <td>3257</td>\n",
              "      <td>...</td>\n",
              "      <td>843679</td>\n",
              "      <td>1528.403986</td>\n",
              "      <td>9793.226830</td>\n",
              "      <td>87089</td>\n",
              "      <td>4147.095238</td>\n",
              "      <td>15826.690943</td>\n",
              "      <td>1259632</td>\n",
              "      <td>59982.476190</td>\n",
              "      <td>58697.726407</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0059420b</td>\n",
              "      <td>785483.026350</td>\n",
              "      <td>785604.874679</td>\n",
              "      <td>121.848329</td>\n",
              "      <td>542.537275</td>\n",
              "      <td>103.618895</td>\n",
              "      <td>1304</td>\n",
              "      <td>1038</td>\n",
              "      <td>1038</td>\n",
              "      <td>1146</td>\n",
              "      <td>...</td>\n",
              "      <td>745392</td>\n",
              "      <td>3067.456790</td>\n",
              "      <td>13400.325753</td>\n",
              "      <td>1466851</td>\n",
              "      <td>112834.692308</td>\n",
              "      <td>365509.369147</td>\n",
              "      <td>1032626</td>\n",
              "      <td>79432.769231</td>\n",
              "      <td>48355.056728</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0075873a</td>\n",
              "      <td>713354.197550</td>\n",
              "      <td>713478.141446</td>\n",
              "      <td>123.943896</td>\n",
              "      <td>600.050968</td>\n",
              "      <td>125.082971</td>\n",
              "      <td>1942</td>\n",
              "      <td>1541</td>\n",
              "      <td>1541</td>\n",
              "      <td>1964</td>\n",
              "      <td>...</td>\n",
              "      <td>1039641</td>\n",
              "      <td>3208.768519</td>\n",
              "      <td>11181.242502</td>\n",
              "      <td>142133</td>\n",
              "      <td>6179.695652</td>\n",
              "      <td>13762.720184</td>\n",
              "      <td>1073221</td>\n",
              "      <td>46661.782609</td>\n",
              "      <td>73950.739406</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 60 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e8fb943-9174-468f-955f-0efcfe0c5cb7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e8fb943-9174-468f-955f-0efcfe0c5cb7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e8fb943-9174-468f-955f-0efcfe0c5cb7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51d5d752-f91d-47e8-a900-fe5e04b88dce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51d5d752-f91d-47e8-a900-fe5e04b88dce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51d5d752-f91d-47e8-a900-fe5e04b88dce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_final1=pd.read_csv('/content/drive/Shareddrives/CSCI567/CSCI567/writing_quality/final_df.csv')\n",
        "# df_final = pd.merge(df_final, df_final1[['id','score']], on='id', how=\"outer\")\n",
        "# df_final.head()"
      ],
      "metadata": {
        "id": "Fx80q7EFO3Vn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Assuming df_final is already defined and has the 'score' column\n",
        "\n",
        "# Define features (X) and labels (y)\n",
        "X = df_final.drop(columns=['id', 'score'])\n",
        "y = df_final['score']\n",
        "X_ids = df_final['id']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Encode the labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting the encoded labels and scaled features into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "HOKD7olar79s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# sc = MinMaxScaler()\n",
        "\n",
        "# X_train = sc.fit_transform(X_train)\n",
        "# X_test = sc.transform(X_test)\n",
        "\n",
        "# # Applying PCA function on training\n",
        "# # and testing set of X component\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# pca = PCA(n_components = 9)\n",
        "\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "# explained_variance = pca.explained_variance_ratio_\n",
        "# print(explained_variance)\n",
        "\n",
        "# sum=0\n",
        "# for i in explained_variance:\n",
        "#     sum+=i\n",
        "\n",
        "# print(sum)"
      ],
      "metadata": {
        "id": "W7xb5FkBMNvL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the split labels and features to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "l = y_train_tensor.unique().size(dim=0)\n",
        "\n",
        "# Display the shapes to confirm the split\n",
        "X_train.shape, X_test.shape, y_train_encoded.shape, y_test_encoded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgz69nhhM5ht",
        "outputId": "56c54120-0a7b-4132-8928-aa890ae8fa67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1976, 58), (495, 58), (1976,), (495,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Defining the model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.linear(x)\n",
        "        # outputs = nn.functional.softmax(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "# Number of features and output classes\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = l\n",
        "\n",
        "# Instantiate the model\n",
        "model = LogisticRegressionModel(input_dim, output_dim)\n",
        "\n",
        "# # Check for cuda\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Move the model to the device\n",
        "# model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    # -(torch.nn.functional.log_softmax(y_pred, dim=1) * y_true).sum(dim=1).mean()\n",
        "    # torch.nn.functional.cross_entropy(input=y_pred, target=y_true)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Display the model parameters\n",
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x_u3KkMtdv-",
        "outputId": "ddd85eef-5b6a-44bd-95dd-e49e735a80a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 2.1000\n",
            "Epoch [200/1000], Loss: 1.9683\n",
            "Epoch [300/1000], Loss: 1.8936\n",
            "Epoch [400/1000], Loss: 1.8414\n",
            "Epoch [500/1000], Loss: 1.8021\n",
            "Epoch [600/1000], Loss: 1.7713\n",
            "Epoch [700/1000], Loss: 1.7462\n",
            "Epoch [800/1000], Loss: 1.7251\n",
            "Epoch [900/1000], Loss: 1.7070\n",
            "Epoch [1000/1000], Loss: 1.6912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[ 5.4722e-02,  8.6035e-02, -2.8766e-02,  8.3697e-02, -2.0347e-01,\n",
              "                        1.3681e-01,  4.7482e-02,  2.1099e-01,  2.9471e-02,  5.7185e-02,\n",
              "                        8.4939e-02, -1.0211e-01,  1.3674e-02,  5.8244e-02, -4.7226e-02,\n",
              "                        4.6297e-02,  1.1282e-01,  1.5653e-02, -6.4555e-02, -1.3724e-02,\n",
              "                        2.7584e-01, -1.9801e-01,  1.2319e-01, -7.0082e-02, -5.7749e-02,\n",
              "                       -5.1069e-02, -6.5864e-03,  3.6299e-02, -1.1001e-01,  9.4047e-02,\n",
              "                        2.5684e-02,  8.8881e-02, -2.0266e-01,  7.5984e-02,  5.7457e-02,\n",
              "                        6.4479e-02,  9.7989e-02, -4.0706e-01,  4.3630e-01,  2.1344e-01,\n",
              "                        5.9753e-02, -2.6522e-01,  4.3193e-01,  8.0831e-02,  1.1255e-01,\n",
              "                       -7.7867e-02,  2.1428e-02, -6.8400e-02,  1.8010e-03, -1.5884e-01,\n",
              "                        7.9736e-02,  1.1302e-01, -1.0824e-01,  1.2571e-01,  2.6139e-02,\n",
              "                        1.1573e-01,  7.0779e-02, -8.6016e-02],\n",
              "                      [ 8.3676e-02, -7.7896e-02, -8.2515e-02,  5.5517e-02,  1.1033e-01,\n",
              "                       -7.8715e-02,  1.7710e-01,  1.4392e-01,  1.0972e-01,  9.4368e-02,\n",
              "                       -1.6783e-01,  1.0464e-01, -9.6969e-02,  7.4467e-02,  2.5655e-02,\n",
              "                       -1.1406e-01, -5.8303e-02,  2.2004e-01,  1.9198e-01, -3.7545e-02,\n",
              "                        1.5530e-01,  1.2467e-02, -1.0261e-02,  1.7076e-02, -6.0251e-01,\n",
              "                        4.6328e-01, -4.2900e-01,  3.1946e-01,  2.1392e-02,  1.3377e-01,\n",
              "                        7.9963e-02,  9.2646e-03,  3.1000e-02, -2.2402e-02,  6.5218e-02,\n",
              "                        1.8266e-01, -8.8751e-02, -2.5926e-01,  3.0411e-01,  4.0418e-01,\n",
              "                       -3.0485e-01, -4.4399e-01,  3.7179e-01,  3.9663e-01, -1.1390e-01,\n",
              "                       -6.6093e-02,  2.5036e-01,  2.1381e-01, -4.6665e-01, -3.2922e-01,\n",
              "                        4.9822e-01,  2.3146e-01, -1.0296e-01,  1.5816e-01,  5.3912e-02,\n",
              "                       -5.3203e-02,  3.1567e-01, -4.1481e-02],\n",
              "                      [ 2.8585e-01,  2.8406e-01, -2.0859e-01,  1.2056e-02, -1.1509e-02,\n",
              "                       -3.2383e-03,  6.1851e-02, -1.0377e-01, -1.0380e-01,  5.6384e-02,\n",
              "                        2.2191e-02,  3.6346e-02,  1.2440e-01, -2.4413e-02, -6.0235e-02,\n",
              "                        5.7681e-02, -1.6034e-01,  3.7918e-02,  4.6664e-02,  2.2431e-02,\n",
              "                       -2.1277e-01, -4.6374e-01,  7.5535e-02,  1.5574e-01, -1.4882e-01,\n",
              "                        1.2009e-02, -2.0419e-01,  3.7433e-01,  1.9413e-03,  2.2061e-02,\n",
              "                       -5.3113e-02, -1.2830e-02,  1.4699e-01, -3.4472e-02,  1.7498e-01,\n",
              "                        1.4081e-01, -2.6935e-01, -3.4918e-01,  5.2455e-01,  3.5514e-02,\n",
              "                        3.6711e-02, -2.9224e-01,  3.2583e-01, -1.0596e-01, -1.4000e-01,\n",
              "                       -1.2380e-01, -5.3860e-02,  1.7437e-01, -8.1666e-03, -1.4715e-01,\n",
              "                        4.1282e-02,  2.3795e-01, -3.6296e-02,  1.8295e-01, -6.7165e-02,\n",
              "                        3.4226e-02, -2.2468e-02,  3.6636e-02],\n",
              "                      [ 1.7105e-01,  2.6967e-01, -4.6955e-02, -1.2743e-01, -3.8527e-01,\n",
              "                       -6.0041e-02,  8.2065e-02,  5.5186e-02, -2.5657e-02,  1.7786e-01,\n",
              "                        2.2686e-01,  2.2592e-02,  1.1495e-01, -1.3392e-01,  2.0139e-01,\n",
              "                        1.4299e-02,  2.8939e-03, -3.9404e-01,  9.6850e-02, -1.0217e-01,\n",
              "                       -1.5714e-01, -4.2247e-01,  2.1804e-02,  4.5023e-01, -1.1913e-01,\n",
              "                        8.5392e-02, -1.9097e-01,  3.0166e-01,  1.1022e-01, -6.8611e-02,\n",
              "                       -1.1529e-01, -1.2629e-02,  2.4814e-03,  9.9782e-02,  9.4515e-02,\n",
              "                        9.3144e-02, -1.4446e-01, -1.0480e-01,  4.6795e-01,  2.2837e-01,\n",
              "                       -8.9913e-02, -4.4366e-01,  3.9968e-01, -7.1414e-02, -3.1160e-01,\n",
              "                       -2.2666e-01,  5.2488e-02,  9.5848e-02, -1.1999e-01, -1.2010e-01,\n",
              "                        3.3751e-01, -1.5388e-01,  5.4384e-02,  2.7763e-01, -1.5887e-01,\n",
              "                       -2.1536e-03,  4.0813e-03,  6.2712e-02],\n",
              "                      [ 1.9473e-01,  1.3838e-01, -1.1736e-01, -1.9038e-01, -5.8901e-02,\n",
              "                       -9.1296e-02, -2.0268e-01, -3.3824e-01, -2.6165e-01,  1.1511e-01,\n",
              "                        3.0281e-01, -2.3995e-01,  4.5692e-01,  2.9091e-01, -4.6843e-01,\n",
              "                        4.7189e-02,  2.0887e-01, -4.0924e-02, -9.5127e-02,  2.8293e-01,\n",
              "                       -1.7424e-01, -2.8337e-01, -1.2365e-01,  2.3182e-01, -3.3082e-01,\n",
              "                        2.2903e-01,  1.0573e-01,  9.5086e-02, -1.8682e-01, -4.2456e-02,\n",
              "                       -1.9429e-01, -6.5619e-02,  7.6688e-02, -2.7453e-01, -1.2009e-01,\n",
              "                       -6.9058e-02, -2.5715e-02,  2.7627e-01,  4.3526e-01, -6.1429e-01,\n",
              "                       -2.0059e-01,  1.9442e-01,  4.0169e-01, -7.1768e-01, -1.6954e-01,\n",
              "                       -3.6158e-01,  1.5598e-01,  1.3458e-01, -2.3549e-01,  4.8655e-02,\n",
              "                        3.0817e-01, -1.1605e-01, -1.6284e-01,  2.4004e-01, -9.2348e-02,\n",
              "                       -4.5005e-02, -3.7947e-02,  1.1004e-01],\n",
              "                      [ 1.3339e-01,  1.4522e-01, -3.1807e-02, -2.0308e-01, -2.4870e-01,\n",
              "                       -1.2661e-01, -1.4402e-01, -2.4266e-01, -1.6483e-01,  1.0701e-01,\n",
              "                        1.8930e-02,  1.3483e-01,  4.5688e-02,  2.0800e-01, -3.9244e-01,\n",
              "                       -2.8713e-01,  1.2851e-01,  7.8957e-02, -5.4790e-05,  2.1304e-01,\n",
              "                       -2.6571e-01, -3.1129e-01, -2.5678e-01,  9.3297e-02, -5.0416e-03,\n",
              "                        2.3192e-01,  6.0124e-02, -1.0545e-02, -2.1233e-02,  1.3162e-03,\n",
              "                       -6.5174e-02, -2.5232e-01, -2.2716e-03,  1.1567e-01,  1.3502e-01,\n",
              "                       -1.0240e-01, -2.9575e-02,  1.2908e-01,  1.3213e-02, -2.2982e-01,\n",
              "                        2.6877e-02,  9.2086e-02, -1.7938e-02, -4.0827e-01, -4.7518e-02,\n",
              "                       -3.6839e-02,  3.9063e-01, -2.2467e-01, -1.2452e-01, -1.8588e-02,\n",
              "                        1.4565e-01,  9.2463e-03,  5.5771e-02,  7.8358e-02,  7.1754e-02,\n",
              "                       -2.1364e-01,  8.5187e-02, -3.6168e-02],\n",
              "                      [-1.1537e-01,  5.0068e-02, -4.1599e-02, -3.2864e-01,  3.6857e-02,\n",
              "                       -1.5123e-01, -1.2193e-01, -1.4533e-01, -6.1351e-02, -8.0587e-02,\n",
              "                       -1.6147e-01,  3.4374e-01,  1.8714e-01,  6.1406e-02, -9.5342e-02,\n",
              "                       -1.6823e-02, -2.6123e-02, -2.1134e-01, -3.6257e-02,  1.4277e-01,\n",
              "                       -1.3979e-01, -2.2970e-01, -1.4613e-01, -2.5480e-01, -3.7493e-02,\n",
              "                       -3.5921e-02,  5.5344e-02, -1.2323e-02, -1.5177e-01, -6.6543e-02,\n",
              "                       -1.4045e-01,  1.5687e-01, -2.0880e-01,  2.1808e-01, -8.2601e-02,\n",
              "                        1.8991e-01, -7.1680e-02,  1.8834e-01, -2.2702e-01,  3.9774e-02,\n",
              "                       -8.0088e-02,  1.6168e-01, -3.2035e-01, -1.1660e-01,  1.5373e-01,\n",
              "                        4.0909e-02, -1.1343e-01,  4.4094e-02,  1.6661e-01,  2.4604e-01,\n",
              "                        4.1866e-02, -2.2202e-01,  1.4070e-01, -8.0363e-02, -6.4223e-02,\n",
              "                        1.3540e-01,  9.4265e-03, -1.2684e-01],\n",
              "                      [-2.5795e-01, -3.2371e-01,  2.7832e-01, -2.4829e-01,  4.9723e-01,\n",
              "                       -3.4893e-01, -1.3108e-01, -3.9308e-02,  1.2498e-01,  9.5418e-02,\n",
              "                       -4.7552e-01,  2.3294e-01,  2.9882e-01, -2.3785e-01,  1.5238e-01,\n",
              "                        1.9312e-02,  9.2501e-02,  1.0678e-01,  1.0415e-01,  1.6627e-02,\n",
              "                        1.2535e-01,  1.2803e-01, -5.0773e-01, -4.1796e-02,  1.5141e-01,\n",
              "                       -2.8026e-03,  8.5995e-02, -2.3143e-01, -4.3153e-01,  1.4604e-01,\n",
              "                        1.6652e-01, -1.1453e-01, -1.8385e-01,  1.7101e-01, -1.6371e-01,\n",
              "                       -2.3788e-01,  1.9556e-01, -2.0688e-01, -5.7789e-01, -2.0010e-01,\n",
              "                        5.0259e-01,  1.4909e-01, -5.7381e-01,  4.1328e-01,  1.6035e-01,\n",
              "                        2.0669e-01, -2.4736e-01, -3.1313e-01,  4.5525e-01,  3.1221e-02,\n",
              "                       -3.7214e-01,  1.8928e-01,  3.7851e-01, -4.3600e-01, -1.0204e-01,\n",
              "                        3.3311e-01, -1.6234e-01, -2.7327e-02],\n",
              "                      [-1.9513e-01, -1.6792e-01, -1.3431e-01,  3.4043e-03,  6.1127e-02,\n",
              "                        7.0228e-03, -2.5713e-02,  6.3845e-02,  2.2096e-01, -2.1125e-01,\n",
              "                       -7.5783e-02, -1.8203e-01, -6.5529e-02, -1.4106e-01,  1.9633e-01,\n",
              "                       -4.2420e-02, -7.4826e-02,  1.6593e-01, -3.6284e-03, -2.1854e-02,\n",
              "                        1.6914e-01,  2.9849e-01,  9.3124e-02, -3.6281e-01,  1.7252e-01,\n",
              "                       -1.6490e-01, -1.4545e-01, -2.2833e-01,  2.1174e-03, -2.2823e-01,\n",
              "                        8.1634e-02,  2.3653e-01, -3.7777e-01,  5.0219e-02,  6.9713e-02,\n",
              "                        1.2190e-01, -1.8008e-01,  3.3827e-01, -3.7472e-01, -1.2175e-01,\n",
              "                       -1.4117e-01,  2.3906e-02, -4.8475e-01, -6.1511e-01,  1.5773e-01,\n",
              "                        3.4478e-02, -2.7266e-01, -1.5398e-02,  2.0189e-01,  1.3326e-01,\n",
              "                       -1.5313e-01, -7.2476e-02,  2.8338e-02, -2.0025e-01,  5.6648e-02,\n",
              "                        4.0859e-01, -2.8990e-01, -2.5658e-02],\n",
              "                      [-4.8589e-02, -1.7158e-01, -2.5803e-02,  1.8700e-01,  1.3990e-01,\n",
              "                        1.9453e-01,  3.9546e-02,  7.5772e-02,  8.0874e-02,  2.1053e-02,\n",
              "                       -1.5318e-01, -1.1293e-01, -1.0682e-01,  2.4520e-02,  1.5873e-01,\n",
              "                        8.5687e-02,  2.2149e-02,  5.3940e-02,  6.4071e-02, -2.9823e-01,\n",
              "                        2.1657e-01,  4.2537e-01,  1.1546e-01, -1.2172e-01,  3.2997e-01,\n",
              "                       -2.8159e-01,  1.9069e-01, -4.6035e-01,  5.4585e-03,  1.2880e-03,\n",
              "                        6.0612e-02, -1.5163e-01,  1.7842e-02, -2.2222e-01, -2.1102e-01,\n",
              "                       -3.5311e-03,  1.0440e-01, -1.5180e-01, -2.1719e-01,  6.1503e-02,\n",
              "                        8.7313e-02,  1.9427e-01, -2.9629e-01, -9.8135e-02,  3.0967e-01,\n",
              "                        1.9509e-01,  2.7616e-01, -2.4774e-01, -1.6444e-01,  1.1370e-01,\n",
              "                       -1.5051e-01,  8.4449e-02,  4.5301e-01, -3.9661e-01, -1.6296e-01,\n",
              "                        9.7414e-02, -1.8170e-01, -8.5797e-02],\n",
              "                      [-1.4563e-01, -1.2997e-01,  1.0207e-01,  3.1476e-01,  1.9601e-01,\n",
              "                        2.0124e-01,  2.4243e-01,  2.2417e-01,  6.4353e-02, -1.6587e-01,\n",
              "                        1.1325e-01, -2.1507e-01, -1.8846e-01, -1.6786e-01,  9.5483e-02,\n",
              "                        5.5161e-02,  1.0167e-01, -6.7962e-02,  4.6217e-02,  2.3132e-01,\n",
              "                        3.9328e-01,  4.6867e-01,  2.8664e-01,  7.3623e-02,  4.8123e-02,\n",
              "                       -9.2271e-02,  3.9357e-02,  3.1792e-02,  2.8550e-02,  1.2346e-01,\n",
              "                       -9.2730e-03,  2.3589e-02,  4.3570e-02, -5.7510e-02,  8.3021e-03,\n",
              "                       -2.1822e-02,  1.7668e-01, -1.9434e-01,  4.0940e-04,  7.6580e-02,\n",
              "                        1.5215e-01, -1.0307e-02,  7.8011e-02,  1.4479e-01,  1.9712e-01,\n",
              "                        1.3679e-01, -3.1720e-01,  1.7716e-01,  1.8794e-01,  3.7441e-02,\n",
              "                       -4.8637e-02, -8.1026e-02, -1.2956e-01,  2.6335e-02,  3.9496e-02,\n",
              "                       -8.3739e-02,  2.0534e-01, -1.0251e-01],\n",
              "                      [-9.2643e-03, -1.0473e-01, -1.3080e-01,  4.9920e-01, -9.2607e-03,\n",
              "                        2.5571e-01,  2.0982e-01,  3.4708e-01, -6.2456e-02, -2.1549e-01,\n",
              "                        8.8260e-02, -1.0316e-01, -2.8881e-01,  2.6729e-01, -1.6170e-01,\n",
              "                        1.9739e-01, -1.3727e-02,  2.1469e-02, -7.8674e-03,  2.2178e-01,\n",
              "                        2.9531e-01,  3.3160e-01,  4.5056e-01,  1.2107e-01,  1.0146e-01,\n",
              "                        1.6433e-01, -2.7773e-01, -3.9035e-01, -7.0802e-03,  7.4075e-02,\n",
              "                       -7.4182e-03, -1.2770e-01,  1.5491e-01,  1.1634e-01,  8.8912e-02,\n",
              "                       -2.0429e-01,  3.3177e-01, -1.7922e-01,  4.3100e-01,  7.9520e-02,\n",
              "                        1.2062e-03, -1.0813e-01,  4.7077e-01,  1.8463e-01, -1.3273e-01,\n",
              "                        8.3414e-02,  5.7816e-02,  4.7231e-02,  5.9991e-02, -1.1447e-01,\n",
              "                        2.4150e-01,  1.4421e-01,  8.1130e-02,  2.4278e-02, -1.2789e-02,\n",
              "                       -7.6188e-02,  5.8384e-02,  1.5549e-01]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.6143, -0.6913, -0.7222, -0.7409, -0.5780,  0.2806,  0.8651,  0.7537,\n",
              "                       0.5456, -0.5361, -0.7326, -0.5892]))])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation phase\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Inference without gradient calculation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += y_test_tensor.size(0)\n",
        "    correct += (predicted == y_test_tensor).sum().item()\n",
        "\n",
        "print(correct)\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
        "print(f'Predicted classes: {predicted}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhKYhC9ctisn",
        "outputId": "b669b0b3-b3f8-4ef1-c519-d24dce8f4f87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157\n",
            "Accuracy of the model on the test set: 31.72%\n",
            "Predicted classes: tensor([ 5,  7,  5,  7,  4,  7,  7,  6,  8,  8,  5,  6,  5,  7,  8,  6,  8,  8,\n",
            "         7,  5,  8,  6,  5, 10,  6,  6,  5,  7,  5,  6,  7,  7,  9,  8, 10,  5,\n",
            "         5,  5,  6,  8,  5,  8,  8,  6,  7,  6,  4,  4,  8,  5,  5, 10,  8,  5,\n",
            "         3,  6,  5,  6,  8,  6,  8,  7,  5,  6,  6,  8,  7,  5,  8,  7,  8,  7,\n",
            "         5,  5,  8,  4,  7,  8,  5,  4,  7, 10,  5,  8,  4,  6,  9,  6,  8,  9,\n",
            "         6,  7,  6,  8,  7,  6,  7, 10,  6,  8,  8,  5,  6,  8,  5,  7,  7,  5,\n",
            "         5,  8,  6,  6,  6,  5,  5,  8,  5,  8,  8,  6,  6,  6,  7,  6, 10,  7,\n",
            "         8,  6,  8,  8,  4,  4,  7,  7,  7,  6,  7,  5,  7,  6,  7,  5,  5,  8,\n",
            "         7,  8,  7,  7, 10,  6,  4,  8,  7,  5,  7,  6,  6,  8,  6,  5,  6,  6,\n",
            "         7,  8,  4, 10,  6,  4,  6,  5,  5,  4, 10,  8,  7,  8,  7,  6,  9, 10,\n",
            "         8,  7,  8, 10,  6,  4,  6,  8,  7,  8,  8,  4,  8, 10, 10,  8,  8,  7,\n",
            "         8,  4,  8,  7,  9,  5,  7, 10,  8,  4,  6,  6,  5,  5,  6,  6,  7,  8,\n",
            "         5,  7, 10,  6,  6,  7,  5,  8,  5,  5,  7,  5,  8,  8,  7,  7,  8, 10,\n",
            "         7,  8,  5,  7,  8,  4,  8,  5,  6,  5,  4,  7,  8,  5,  8,  6,  7,  7,\n",
            "         6,  5,  5,  4,  8,  5,  7,  6,  8,  6,  5,  5,  7, 10,  4,  4,  4,  8,\n",
            "         6,  7,  5,  6,  4,  5,  7,  8,  8,  7,  8,  6,  7,  5,  2,  5,  8,  6,\n",
            "         5,  5,  4,  8,  4,  7,  7,  8,  6,  7,  8,  8,  4,  7,  4,  7,  7,  8,\n",
            "         8,  6,  7, 10,  7,  8,  4,  8,  7,  7,  8,  5,  7,  7,  8,  5,  5,  8,\n",
            "         6,  5,  5,  8,  7,  5,  5,  8,  5,  6,  8,  7,  6,  2,  6,  6,  6,  8,\n",
            "         5,  8,  6, 10,  4,  7,  6,  4,  6,  5,  4,  4,  7,  5,  7,  4, 10,  7,\n",
            "         8,  8,  7,  5,  8,  8,  4,  3,  5,  8,  7,  8,  7,  4,  7,  5,  8,  7,\n",
            "         8,  6,  7, 10,  5,  6,  6,  6,  4,  6,  8,  8,  7,  8,  7,  8,  7,  5,\n",
            "         8,  6,  4,  8,  8,  4,  8,  5,  6,  6,  7,  5,  8,  8,  4,  4,  8,  5,\n",
            "         8,  5,  6,  5,  7,  6,  8,  8,  7,  7,  8, 10,  8,  7,  7,  5,  7,  4,\n",
            "         7,  8,  5,  6,  6,  7,  8, 10,  7,  5,  5,  7,  8,  8,  7,  4,  8,  5,\n",
            "         8,  7,  5,  7,  5,  8,  6,  6,  6,  6,  8,  7,  5,  7,  5,  6,  6,  7,\n",
            "         7,  8,  6,  5,  6,  6,  7,  8,  6,  7,  6,  4,  5,  8,  9,  6,  8,  5,\n",
            "         7,  6,  6,  6,  6,  5,  7,  6,  6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert predicted classes back to original scores using the inverse transform method of label_encoder\n",
        "predicted_scores = label_encoder.inverse_transform(predicted.numpy())\n",
        "y_scores = label_encoder.inverse_transform(y_test_encoded)\n",
        "np.sqrt(np.mean((predicted_scores-y_scores)**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtozYVEotzbY",
        "outputId": "bcf41e4b-8f4c-41cd-9583-6b9f2fb894ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6834996102206037"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################ORDINAL####################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class OrdinalLogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(OrdinalLogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim - 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        return logits\n",
        "\n",
        "def ordinal_logistic_loss(outputs, target, num_classes):\n",
        "    # Creating a binary matrix for cumulative probabilities\n",
        "    target = torch.nn.functional.one_hot(target, num_classes).cumsum(dim=1)[:, :-1].float()\n",
        "    return torch.nn.functional.binary_cross_entropy_with_logits(outputs, target)\n",
        "\n",
        "# Instantiate the ordinal logistic regression model\n",
        "model = OrdinalLogisticRegressionModel(input_dim, output_dim)\n",
        "\n",
        "# Use the custom ordinal logistic loss function\n",
        "criterion = ordinal_logistic_loss\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor, 12)\n",
        "    # -(torch.nn.functional.log_softmax(y_pred, dim=1) * y_true).sum(dim=1).mean()\n",
        "    # torch.nn.functional.cross_entropy(input=y_pred, target=y_true)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Evaluation phase\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    outputs = model(X_test_tensor)\n",
        "    predicted = torch.sigmoid(outputs).cumsum(dim=1).argmax(dim=1)\n",
        "    total += y_test_tensor.size(0)\n",
        "    correct += (predicted == y_test_tensor).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
        "\n",
        "# Converting predicted classes back to original scores\n",
        "predicted_scores = label_encoder.inverse_transform(predicted.numpy())\n",
        "y_scores = label_encoder.inverse_transform(y_test_encoded)\n",
        "np.sqrt(np.mean((predicted_scores - y_scores)**2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH2MNvEBClmz",
        "outputId": "bf745a08-db5a-498d-aac7-283506bf9c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.6018\n",
            "Epoch [200/1000], Loss: 0.5589\n",
            "Epoch [300/1000], Loss: 0.5263\n",
            "Epoch [400/1000], Loss: 0.4985\n",
            "Epoch [500/1000], Loss: 0.4740\n",
            "Epoch [600/1000], Loss: 0.4520\n",
            "Epoch [700/1000], Loss: 0.4321\n",
            "Epoch [800/1000], Loss: 0.4141\n",
            "Epoch [900/1000], Loss: 0.3978\n",
            "Epoch [1000/1000], Loss: 0.3829\n",
            "Accuracy of the model on the test set: 5.25%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.03529463282942"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1h0AitERsIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}